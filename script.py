import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import sys

# --- CONFIGURA√á√ÉO ---
POEM_BOOK_PATH = 'poems.txt' # Novo caminho para o arquivo TXT
MAX_FEATURES = 5000 

# --- FUN√á√ïES DE PR√â-PROCESSAMENTO ---

def basic_clean(text):
    """Aplica limpeza b√°sica: min√∫sculas, remove n√£o-alfanum√©ricos (exceto espa√ßo) e espa√ßos extras."""
    if not isinstance(text, str):
        return ""
    text = text.lower()
    # Mantemos pontua√ß√£o por enquanto para ajudar na tokeniza√ß√£o de frases
    text = re.sub(r'[^a-z\s.,!?;:]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def split_and_pair_text(text):
    """
    Divide o texto em frases e cria pares de Query (Frase N) e Response (Frase N+1).
    Usamos regex para dividir por pontua√ß√£o final (. ! ?) seguido de espa√ßo.
    """
    # Adiciona um espa√ßo para garantir que a pontua√ß√£o n√£o fique grudada na palavra
    text = text.replace('.', '. ').replace('!', '! ').replace('?', '? ')
    
    # Remove espa√ßos duplicados ap√≥s a corre√ß√£o
    text = re.sub(r'\s+', ' ', text)

    # Tokeniza em frases usando pontua√ß√µes finais como delimitadores
    # O re.split usa a pontua√ß√£o como delimitador, e o re.sub antes garante o espa√ßo
    # Filtra entradas vazias
    sentences = [s.strip() for s in re.split(r'[.!?]\s*', text) if s.strip()]

    data = []
    # Cria pares de (Frase N, Frase N+1)
    for i in range(len(sentences) - 1):
        data.append({
            'Query': sentences[i],
            'Response': sentences[i+1]
        })
    return pd.DataFrame(data)

def load_and_preprocess_data(path):
    """Carrega o arquivo TXT e prepara o DataFrame de Q/A."""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            full_text = f.read()
    except FileNotFoundError:
        print(f"Erro: Arquivo {path} n√£o encontrado. Certifique-se de que o arquivo TXT est√° no mesmo diret√≥rio.")
        sys.exit(1)

    # 1. Limpeza b√°sica do texto
    cleaned_full_text = basic_clean(full_text)
    
    # 2. Split e cria√ß√£o dos pares de Q/A
    df = split_and_pair_text(cleaned_full_text)
    
    # 3. Criar a coluna de Query limpa (para o TF-IDF)
    # Aqui removemos a pontua√ß√£o para a vetoriza√ß√£o ser mais eficaz
    df['Clean_Query'] = df['Query'].apply(lambda t: re.sub(r'[^\w\s]', '', t))
    
    print(f"Texto carregado e transformado em {len(df)} pares de Query/Response.")
    return df

def train_vectorizer(df):
    """Treina o vetorizador TF-IDF e transforma as queries."""
    print("Treinando o vetorizador TF-IDF...")
    # Usando a lista de stopwords em ingl√™s do scikit-learn
    vectorizer = TfidfVectorizer(stop_words='english', max_features=MAX_FEATURES)
    
    # Treinar e transformar o dataset de queries limpas
    tfidf_matrix = vectorizer.fit_transform(df['Clean_Query'])
    
    print(f"Matriz TF-IDF treinada. Dimens√µes: {tfidf_matrix.shape}")
    return vectorizer, tfidf_matrix

# --- L√ìGICA DO CHATBOT ---

def get_response(query, vectorizer, tfidf_matrix, df):
    """Busca a resposta mais adequada com base na Similaridade de Cosseno."""
    
    # 1. Pr√©-processar a query do usu√°rio (primeira limpeza e depois remover pontua√ß√£o para vetoriza√ß√£o)
    clean_query_basic = basic_clean(query)
    clean_query_vectorized = re.sub(r'[^\w\s]', '', clean_query_basic)
    
    if not clean_query_vectorized:
        return "O eco das palavras n√£o me alcan√ßa. Fale mais claramente."

    # 2. Transformar a query em vetor TF-IDF
    query_vector = vectorizer.transform([clean_query_vectorized])

    # 3. Calcular a Similaridade de Cosseno
    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()

    # 4. Encontrar o √≠ndice da query mais similar
    best_match_index = np.argmax(cosine_similarities)
    
    # 5. Definir um limite m√≠nimo de similaridade
    similarity_threshold = 0.15 
    
    best_similarity = cosine_similarities[best_match_index]

    if best_similarity < similarity_threshold:
        return "Sua pergunta paira em mist√©rio. O livro n√£o me deu palavras para isso."
    
    # 6. Extrair a resposta (Frase N+1)
    # matched_query = df['Query'].iloc[best_match_index] # Para debug
    chatbot_response = df['Response'].iloc[best_match_index]
    
    return chatbot_response

# --- FUN√á√ÉO PRINCIPAL DE EXECU√á√ÉO ---

def run_chatbot():
    """Executa o loop principal de conversa√ß√£o."""
    df = load_and_preprocess_data(POEM_BOOK_PATH)
    vectorizer, tfidf_matrix = train_vectorizer(df)
    
    print("\n" + "="*50)
    print("üìú POEM CHATBOT (Modelo de Estilo e Tom)")
    print("Tente iniciar uma frase do livro. Diga 'sair' para encerrar.")
    print("="*50 + "\n")

    # Iniciar o loop de chat
    while True:
        try:
            user_input = input("Voc√™: ")
            
            if user_input.lower() in ['sair', 'exit', 'quit']:
                print("\nü§ñ Chatbot: As palavras silenciam. Adeus. Fim da sess√£o.")
                break
                
            response = get_response(user_input, vectorizer, tfidf_matrix, df)
            
            # Formatar a resposta
            print(f"üí¨ Resposta: {response}")

        except EOFError:
            print("\nü§ñ Chatbot: Fim da sess√£o.")
            break
        except Exception as e:
            print(f"Ocorreu um erro: {e}")
            break

# 7. Execu√ß√£o do Chatbot (chamada principal)
print("O novo Chatbot de Poemas est√° pronto. Execute o arquivo 'poem_chatbot.py' no seu ambiente local!")
# run_chatbot() # Descomente para executar localmente
